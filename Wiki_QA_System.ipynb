{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CST 4802 Information Retrieval Final Project\n",
        "\n",
        "\n",
        "Option #5: Build a Question-Answering System (QA)"
      ],
      "metadata": {
        "id": "AqQGOjpwFBWa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXWpHcXRY-6b",
        "outputId": "c7714f16-2c00-4b4d-8ff9-13aa1b0bd2bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wikipedia-api in /usr/local/lib/python3.10/dist-packages (0.7.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.3)\n",
            "Requirement already satisfied: rank-bm25 in /usr/local/lib/python3.10/dist-packages (0.2.2)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.3.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.1)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from wikipedia-api) (2.32.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.5.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.6.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.27.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (11.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.2.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.9.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (17.0.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.11.10)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (2024.12.14)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.17.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "%pip install wikipedia-api nltk gensim rank-bm25 sentence-transformers transformers evaluate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wikipediaapi\n",
        "\n",
        "def fetch_wikipedia_pages(topics, lang='en'):\n",
        "    \"\"\"\n",
        "    Fetch content for multiple Wikipedia topics.\n",
        "    :param topics: List of Wikipedia page titles\n",
        "    :param lang: Language code ('en' for English)\n",
        "    :return: Dictionary of page title -> page content\n",
        "    \"\"\"\n",
        "    user_agent = \"MyWikipediaBot/1.0 (khansilma@gmail.com)\"\n",
        "    wiki_wiki = wikipediaapi.Wikipedia(language=lang, user_agent=user_agent)\n",
        "    pages = {}\n",
        "\n",
        "    for topic in topics:\n",
        "        page = wiki_wiki.page(topic)\n",
        "        if page.exists():\n",
        "            pages[topic] = page.text\n",
        "            print(f\"Fetched '{topic}' successfully!\")\n",
        "        else:\n",
        "            print(f\"Page '{topic}' does not exist.\")\n",
        "\n",
        "    return pages\n",
        "\n",
        "topics = ['Information retrieval', 'Natural language processing', 'Artificial intelligence']\n",
        "wiki_corpus = fetch_wikipedia_pages(topics)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9XRI_ZmQd-T_",
        "outputId": "3d3cb48a-3d57-45f5-d6f7-52a696a7831b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetched 'Information retrieval' successfully!\n",
            "Fetched 'Natural language processing' successfully!\n",
            "Fetched 'Artificial intelligence' successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Preprocess text by cleaning and tokenizing sentences without being too aggressive.\n",
        "    :param text: Raw text\n",
        "    :return: List of cleaned sentences\n",
        "    \"\"\"\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    #Clean up sentences (keep punctuation and structure)\n",
        "    cleaned_sentences = []\n",
        "    for sent in sentences:\n",
        "        #Remove unnecessary whitespace and special characters\n",
        "        clean_sent = re.sub(r'\\s+', ' ', sent)  #Replace multiple spaces with a single space\n",
        "        clean_sent = re.sub(r'[^\\x00-\\x7F]+', '', clean_sent)  #Remove non-ASCII characters\n",
        "        cleaned_sentences.append(clean_sent.strip())\n",
        "\n",
        "    return cleaned_sentences\n",
        "\n",
        "for topic, content in wiki_corpus.items():\n",
        "    print(f\"Processing '{topic}'...\")\n",
        "    wiki_corpus[topic] = preprocess_text(content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pmga9FOViR6x",
        "outputId": "d363f80c-664b-4b62-c87c-8ab3be7a23f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing 'Information retrieval'...\n",
            "Processing 'Natural language processing'...\n",
            "Processing 'Artificial intelligence'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from rank_bm25 import BM25Okapi\n",
        "\n",
        "def build_bm25_index(corpus):\n",
        "    \"\"\"\n",
        "    Build a BM25 index using the preprocessed corpus.\n",
        "    :param corpus: Dictionary {title -> list of sentences}\n",
        "    :return: BM25 index and corresponding sentences\n",
        "    \"\"\"\n",
        "    sentences = []\n",
        "    for title, content in corpus.items():\n",
        "        sentences.extend(content)\n",
        "\n",
        "    tokenized_corpus = [sent.split() for sent in sentences]\n",
        "    # bm25 = BM25Okapi(tokenized_corpus)\n",
        "    bm25 = BM25Okapi(tokenized_corpus, k1=1.5, b=0.75)  #Experiment with these values to see how the ranking changes\n",
        "\n",
        "    return bm25, sentences\n",
        "\n",
        "bm25, all_sentences = build_bm25_index(wiki_corpus)"
      ],
      "metadata": {
        "id": "sRJ0Yz-6idlz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "import torch\n",
        "\n",
        "#Load a pre-trained Sentence Transformer model\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "def answer_query_hybrid(query, bm25, sentences, top_n=3):\n",
        "    \"\"\"\n",
        "    Retrieve top N answers to a query using BM25 and refine them with semantic similarity.\n",
        "    :param query: User query (string)\n",
        "    :param bm25: BM25 index\n",
        "    :param sentences: List of sentences\n",
        "    :param top_n: Number of answers to retrieve\n",
        "    :return: List of top N relevant sentences\n",
        "    \"\"\"\n",
        "    tokenized_query = query.lower().split()\n",
        "    scores = bm25.get_scores(tokenized_query)\n",
        "    top_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:10]  #Get the top 10 candidates\n",
        "    candidate_sentences = [sentences[i] for i in top_indices]\n",
        "\n",
        "    query_embedding = model.encode(query, convert_to_tensor=True)\n",
        "    candidate_embeddings = model.encode(candidate_sentences, convert_to_tensor=True)\n",
        "    similarities = util.pytorch_cos_sim(query_embedding, candidate_embeddings)[0]\n",
        "\n",
        "    ranked_indices = torch.argsort(similarities, descending=True)[:top_n]\n",
        "    top_answers = [candidate_sentences[i] for i in ranked_indices]\n",
        "    return top_answers"
      ],
      "metadata": {
        "id": "OyguxtiYigJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Query for Natural language processing\n",
        "query = \"What is natural language processing?\"\n",
        "answers = answer_query_hybrid(query, bm25, all_sentences)\n",
        "print(\"Top Answers:\")\n",
        "for i, ans in enumerate(answers, 1):\n",
        "    print(f\"{i}. {ans}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCqWySg3rhyA",
        "outputId": "f3426899-e384-4017-f916-bbd2e0a0424d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top Answers:\n",
            "1. Major tasks in natural language processing are speech recognition, text classification, natural-language understanding, and natural-language generation.\n",
            "2. Though natural language processing tasks are closely intertwined, they can be subdivided into categories for convenience.\n",
            "3. Natural language understanding involves the identification of the intended semantic from the multiple possible semantics which can be derived from a natural language expression which usually takes the form of organized notations of natural language concepts.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Query for information retrieval\n",
        "query = \"What is information retrieval?\"\n",
        "answers = answer_query_hybrid(query, bm25, all_sentences)\n",
        "print(\"Top Answers:\")\n",
        "for i, ans in enumerate(answers, 1):\n",
        "    print(f\"{i}. {ans}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yeuU7RKY10OJ",
        "outputId": "d59f591b-3d74-43c8-8d69-af45778d1a45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top Answers:\n",
            "1. Information retrieval (IR) in computing and information science is the task of identifying and retrieving information system resources that are relevant to an information need.\n",
            "2. Automated information retrieval systems are used to reduce what has been called information overload.\n",
            "3. Performance and correctness measures The evaluation of an information retrieval system' is the process of assessing how well a system meets the information needs of its users.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#query for artoficial intelligence\n",
        "query = \"What is artificial intelligence?\"\n",
        "answers = answer_query_hybrid(query, bm25, all_sentences)\n",
        "print(\"Top Answers:\")\n",
        "for i, ans in enumerate(answers, 1):\n",
        "    print(f\"{i}. {ans}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_oZJ2TRz2lEm",
        "outputId": "72523997-6e87-4a2b-df82-a959b456a41b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top Answers:\n",
            "1. Dick considers the idea that our understanding of human subjectivity is altered by technology created with artificial intelligence.\n",
            "2. Already in 1950, Alan Turing published an article titled \"Computing Machinery and Intelligence\" which proposed what is now called the Turing test as a criterion of intelligence, though at the time that was not articulated as a problem separate from artificial intelligence.\n",
            "3. Several works use AI to force us to confront the fundamental question of what makes us human, showing us artificial beings that have the ability to feel, and thus to suffer.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_queries = [\n",
        "    {\"query\": \"What is natural language processing?\", \"expected_answers\": [\"natural language understanding\", \"text classification\"]},\n",
        "    {\"query\": \"What is information retrieval?\", \"expected_answers\": [\"identifying and retrieving information\"]},\n",
        "    {\"query\": \"What is artificial intelligence?\", \"expected_answers\": [\"Turing test\", \"intelligence\"]}\n",
        "]"
      ],
      "metadata": {
        "id": "kKimTcPk5kGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "import numpy as np\n",
        "\n",
        "evaluation_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "def evaluate_model(test_queries, bm25, all_sentences, top_n=3):\n",
        "    \"\"\"\n",
        "    Evaluate the question-answering model.\n",
        "    :param test_queries: List of test queries with expected answers\n",
        "    :param bm25: BM25 index\n",
        "    :param all_sentences: List of sentences in the corpus\n",
        "    :param top_n: Number of top answers to retrieve\n",
        "    :return: Average precision, recall, and F1-score\n",
        "    \"\"\"\n",
        "    precisions = []\n",
        "    recalls = []\n",
        "\n",
        "    for test_query in test_queries:\n",
        "        query = test_query[\"query\"]\n",
        "        expected_answers = test_query[\"expected_answers\"]\n",
        "\n",
        "        predicted_answers = answer_query_hybrid(query, bm25, all_sentences, top_n=top_n)\n",
        "\n",
        "        expected_embeddings = evaluation_model.encode(expected_answers, convert_to_tensor=True)\n",
        "        predicted_embeddings = evaluation_model.encode(predicted_answers, convert_to_tensor=True)\n",
        "\n",
        "        similarity_matrix = util.pytorch_cos_sim(expected_embeddings, predicted_embeddings)\n",
        "\n",
        "        threshold = 0.7\n",
        "        relevant_count = (similarity_matrix > threshold).sum().item()\n",
        "        precision = relevant_count / len(predicted_answers) if predicted_answers else 0\n",
        "        recall = relevant_count / len(expected_answers) if expected_answers else 0\n",
        "\n",
        "        precisions.append(precision)\n",
        "        recalls.append(recall)\n",
        "\n",
        "    avg_precision = np.mean(precisions)\n",
        "    avg_recall = np.mean(recalls)\n",
        "    f1_score = 2 * (avg_precision * avg_recall) / (avg_precision + avg_recall) if (avg_precision + avg_recall) > 0 else 0\n",
        "\n",
        "    return avg_precision, avg_recall, f1_score\n",
        "\n",
        "test_queries = [\n",
        "    {\"query\": \"What is natural language processing?\", \"expected_answers\": [\"natural language\", \"text classification\"]},\n",
        "    {\"query\": \"What is information retrieval?\", \"expected_answers\": [\"identifying and retrieving information\"]},\n",
        "    {\"query\": \"What is artificial intelligence?\", \"expected_answers\": [\"Turing test\", \"intelligence\"]}\n",
        "]\n",
        "\n",
        "precision, recall, f1_score = evaluate_model(test_queries, bm25, all_sentences)\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1_score:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l991mBKi5n1Q",
        "outputId": "6ba60e16-b1c6-448f-a2fc-aac9f90919b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.0000\n",
            "Recall: 0.0000\n",
            "F1-Score: 0.0000\n"
          ]
        }
      ]
    }
  ]
}